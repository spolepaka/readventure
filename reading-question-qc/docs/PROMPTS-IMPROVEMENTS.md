# Prompts Improvements Documentation

**Date:** December 10, 2024  
**File Updated:** `ck_gen - prompts.json`

## Overview

This document describes the improvements made to `ck_gen - prompts.json` to close the gaps between question generation prompts and QC (Quality Control) requirements. Previously, questions generated by the system could fail QC checks because the generation prompts didn't instruct the LLM to follow all criteria that would later be checked.

---

## Gap Analysis Summary

### Before Improvements

| QC Check | In Generation Prompt? | Gap Status |
|----------|----------------------|------------|
| `grammatical_parallel` | ✅ Yes | No gap |
| `plausibility` | ✅ Yes | No gap |
| `homogeneity` | ❌ **NOT MENTIONED** | **GAP** |
| `specificity_balance` | ❌ **NOT MENTIONED** | **GAP** |
| `too_close` | ❌ **NOT IN ck_gen** | **GAP** |
| `difficulty_assessment` | ❌ **NOT IN ck_gen** | **GAP** |
| `clarity_precision` | ✅ Yes | No gap |
| `text_dependency` | ✅ Yes | No gap |
| `single_correct_answer` | ✅ Yes | No gap |
| `passage_reference` | ⚠️ Implicit | Minor gap |
| Length check (too short) | ❌ Only checked "too long" | **GAP** |
| Semantic distance | ❌ **NOT MENTIONED** | **GAP** |

### After Improvements

All gaps have been closed. ✅

---

## Changes Made

### 1. Enhanced MCQ Generation Prompts (DOK 1, 2, 3)

Added comprehensive "Quality Requirements" section with the following new criteria:

#### Homogeneity Requirement (NEW)
```
8. **Homogeneity**: All choices MUST belong to the same conceptual category
   - If asking about word meaning → all choices must be possible definitions
   - If asking about a character → all choices must be character-related (traits, actions, feelings)
   - If asking about main idea → all choices must be potential main ideas
   - Do NOT mix character traits with setting details, or themes with specific facts
```

#### Specificity Balance Requirement (NEW)
```
9. **Specificity Balance**: All choices MUST have similar levels of detail
   - GOOD: "happy", "sad", "angry", "worried" (all single emotion words)
   - BAD: "sad" vs "experiencing deep melancholy" vs "feeling blue" (different detail levels)
   - Do NOT have one overly specific choice among general ones
```

#### Enhanced Length Balance (IMPROVED)
```
10. **Length Balance**: Choices should maintain similar character length
    - The correct answer must NOT be the longest option
    - The correct answer must NOT be significantly shorter than all distractors (aim for 70-110% of distractor lengths)
```
*Previously only checked "not longest", now also checks "not too short"*

#### Semantic Distance Requirement (NEW)
```
11. **Semantic Distance**: Distractors must NOT be synonyms or near-synonyms of the correct answer
    - AVOID: correct="happy" with distractor="joyful" (too similar)
    - Each distractor should represent a distinctly different concept
```

### 2. Enhanced MP (Multipart) Generation Prompts (DOK 2, 3)

Added the same quality requirements to both Part A and Part B:
- Homogeneity check for each part
- Specificity balance for each part
- Length balance for each part
- Semantic distance requirements

### 3. Added Quality Verification to Output Format

Each MCQ and MP generation prompt now requests explicit verification in the JSON output:

```json
"quality_verification": {
  "homogeneity_check": "all choices are [category type]",
  "specificity_check": "all choices are at [detail level]",
  "semantic_distance_check": "each option represents a distinct concept"
}
```

This forces the LLM to explicitly think about and verify these requirements before outputting.

### 4. Added New QC Prompts

#### `too_close` Check (NEW)
Evaluates whether any distractors are semantically too close to the correct answer:

```
A distractor is TOO CLOSE if:
1. Synonymous or Near-Synonymous: Uses different wording but same meaning
2. Degree-Only Difference: Differs only in degree/intensity
3. Equally Text-Supported: Both options equally supported by passage
4. Double-Key Risk: Two options could both be considered correct
5. Grade-Inappropriate Distinction: Requires knowledge beyond grade level
```

#### `difficulty_assessment` Check (NEW)
Evaluates whether question difficulty matches the target grade level:

```
Assessment Dimensions:
1. Level of Inference Required (LOW/MODERATE/HIGH)
2. Distractor Difficulty (WEAK/PLAUSIBLE/STRONG)
3. Vocabulary Complexity
4. Cognitive Demand vs stated DOK level

Grade Level Expectations:
- Grades 3-5: Concrete concepts, explicit information, basic inferences
- Grades 6-8: Abstract concepts, implicit information, moderate inferences
- Grades 9-10: Complex analysis, synthesis, sophisticated reasoning
```

### 5. Enhanced SR (Short Response) Prompts

Added explicit `Standard Alignment` requirement to all SR prompts:
```
6. Standard Alignment: Must directly assess the target standard's specific skill
```

---

## Impact on Question Generation

### Before
- LLM might generate questions where choices mixed different concept types
- LLM might generate questions where one choice was much more specific than others
- LLM might generate distractors that were synonyms of the correct answer
- Correct answer might be suspiciously shorter than all distractors
- Questions might fail QC checks that weren't mentioned in generation prompts

### After
- LLM is explicitly instructed to ensure homogeneity of choices
- LLM is explicitly instructed to balance specificity levels
- LLM is explicitly warned against too-close distractors
- Length balance now covers both "too long" and "too short" cases
- LLM must verify quality requirements before outputting
- Two new QC checks (`too_close`, `difficulty_assessment`) available

---

## Recommended Next Steps

### 1. Enable All QC Checks in `bulk_question_generator.py`

The following checks are currently commented out or missing. Enable them for complete QC:

```python
# In run_quality_control() method, update checks_to_run:
checks_to_run = [
    'grammatical_parallel',
    'plausibility', 
    'homogeneity',
    'specificity_balance',
    'too_close',              # NEW - add this
    'standard_alignment',     # Uncomment this
    'clarity_precision',
    'text_dependency',        # Uncomment this
    'single_correct_answer',
    'passage_reference',
    'difficulty_assessment'   # NEW - add this
]
```

### 2. Sync with `qc_pipeline/config/prompts.json`

The `qc_pipeline/` uses a separate prompts file. Consider:
- Using a single source of truth for prompts
- Or ensuring both files stay in sync

### 3. Regenerate Existing Questions

Questions generated before these improvements may fail the new/enhanced QC checks. Consider:
- Running QC on existing questions
- Regenerating questions that fail

---

## 6. Dynamic Grade Level Extraction (NEW - December 10, 2024)

### Problem

Previously, all generation prompts had **hardcoded "grades 9-10"** for grade level references. This was incorrect when generating questions for other grades (e.g., grade 3 standards like `RL.3.1`).

### Solution

1. **Replaced hardcoded grade level with `{grade_level}` placeholder** in all generation prompts.

2. **Added `_extract_grade_level()` helper function** to both `bulk_question_generator.py` and `question_generator.py`.

### How Grade Level is Extracted

The grade level is automatically extracted from the CCSS standard code:

| Standard Code | Extracted Grade Level |
|---------------|----------------------|
| `RL.3.1` | "grade 3" |
| `RI.5.2` | "grade 5" |
| `RL.K.1` | "kindergarten" |
| `RL.9-10.3` | "grades 9-10" |
| `RI.11-12.4` | "grades 11-12" |

### Code Implementation

```python
def _extract_grade_level(self, standard_code: str) -> str:
    """
    Extract grade level from CCSS standard code.
    
    Examples:
        RL.3.1 → "grade 3"
        RI.5.2 → "grade 5"
        RL.9-10.3 → "grades 9-10"
        RI.11-12.4 → "grades 11-12"
        RL.K.1 → "kindergarten"
    """
    if not standard_code:
        return "grade 3"  # Default fallback
    
    parts = standard_code.split('.')
    if len(parts) < 2:
        return "grade 3"  # Default fallback
    
    grade_part = parts[1]
    
    # Handle kindergarten
    if grade_part.upper() == 'K':
        return "kindergarten"
    
    # Handle grade bands (e.g., "9-10", "11-12")
    if '-' in grade_part:
        return f"grades {grade_part}"
    
    # Handle single grades
    return f"grade {grade_part}"
```

### Impact

- **Before**: Questions for `RL.3.1` would be generated with prompts saying "grades 9-10 reading assessment" - potentially creating age-inappropriate vocabulary or concepts.
  
- **After**: Questions for `RL.3.1` will use "grade 3 reading assessment" - ensuring vocabulary and concepts are grade-appropriate.

### Files Updated

| File | Changes |
|------|---------|
| `ck_gen - prompts.json` | Replaced all "grades 9-10" with `{grade_level}` placeholder |
| `bulk_question_generator.py` | Added `_extract_grade_level()` method; Updated `_fill_prompt_variables()` to include `grade_level` |
| `question_generator.py` | Added `_extract_grade_level()` method; Updated `_fill_prompt_variables()` to include `grade_level` |

---

## File Changes Summary

| File | Changes |
|------|---------|
| `ck_gen - prompts.json` | Added quality requirements to MCQ DOK 1/2/3 and MP DOK 2/3 generation prompts; Added `too_close` and `difficulty_assessment` QC prompts; Enhanced length balance requirements; **Replaced hardcoded "grades 9-10" with `{grade_level}` placeholder** |
| `bulk_question_generator.py` | **Added `_extract_grade_level()` method; Updated `_fill_prompt_variables()` to include dynamic grade level** |
| `question_generator.py` | **Added `_extract_grade_level()` method; Updated `_fill_prompt_variables()` to include dynamic grade level** |

---

## Related Documentation

**Note:** The file `PROMPT-COMPARISON.md` in this folder contains the **OLD prompts** before these improvements. It should be considered outdated and may be regenerated from the updated `ck_gen - prompts.json` file.

---

## Appendix: Full List of QC Checks Now Available

### Distractor-Level Checks
1. `grammatical_parallel` - Consistent grammatical structure
2. `plausibility` - Believable wrong answers
3. `homogeneity` - Same conceptual category
4. `specificity_balance` - Similar detail levels
5. `too_close` - **NEW** - Semantic distance from correct answer

### Question-Level Checks
6. `standard_alignment` - Assesses assigned standard
7. `clarity_precision` - Clear and unambiguous
8. `text_dependency` - Requires reading passage
9. `single_correct_answer` - Exactly one correct answer
10. `passage_reference` - Accurate passage references
11. `difficulty_assessment` - **NEW** - Grade-appropriate difficulty

### Question-Set Level Checks
12. `skill_integration` - Multiple competencies covered

### Programmatic Checks (in code, not prompts)
13. `length_check` - Character/word count balance (enhanced: now checks too short)

